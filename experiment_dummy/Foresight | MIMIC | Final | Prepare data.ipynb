{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "01ae6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import datasets\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "# from foresight.datasets import patient_concept_stream\n",
    "# from foresight.datasets.filters import filter_by_count, filter_by_type\n",
    "# from foresight.datasets.utils import get_embeddings_for_tokens, stream_to_separate_examples, add_to_stream, \\\n",
    "#                                   remove_parents_from_stream, bucket_concepts, cleanup_stream, \\\n",
    "#                                   split_stream, add_age, get_all_splits, add_ttd, add_position_ids\n",
    "# from foresight.utils import pickle\n",
    "# from foresight.utils.cdb_utils import get_parents_map \n",
    "# from foresight.utils.stream_utils import docs2stream, calculate_counts\n",
    "# from foresight.tokenizers.simple_map_tokenizer import SimpleMapTokenizer\n",
    "# from foresight.metrics.next_concept_prediction import precision, metrics_data2df, ComputePrecisionHF\n",
    "# from foresight.utils import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9c38222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import Random\n",
    "from pydantic import BaseModel\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c930ab",
   "metadata": {},
   "source": [
    "## Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4b27dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TIMELINES = 1000\n",
    "LETTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "NUM_TIMESTEPS = 10\n",
    "RANDOM_SEED = Random(23)\n",
    "MAX_SKIP = 10\n",
    "MAX_NUM_SAMPLES = 3\n",
    "\n",
    "SEQUENCE_LENGTH = 12*5\n",
    "SEPARATOR = \"<SEP>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d620c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummySample(BaseModel):\n",
    "    timeline: list[str]\n",
    "    gap: int\n",
    "    num_samples: int\n",
    "    uppercase: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8487ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_letters = [l_1+l_2 for l_1 in LETTERS for l_2 in LETTERS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0faa734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    for _ in range(1000):\n",
    "        start_idx = RANDOM_SEED.randint(0, len(double_letters) - 1)\n",
    "        skip_odd = RANDOM_SEED.choice([True, False])\n",
    "        num_samples = RANDOM_SEED.randint(1, MAX_NUM_SAMPLES)\n",
    "\n",
    "        timeline = []\n",
    "        for seq_idx in range(SEQUENCE_LENGTH):\n",
    "            char_idx = start_idx + num_samples*seq_idx\n",
    "            if skip_odd and char_idx % 2 == 1:\n",
    "                timeline.append([])\n",
    "            elif char_idx+num_samples >= len(double_letters):\n",
    "                timeline.append([])\n",
    "            else:\n",
    "                timeline.append(double_letters[char_idx:char_idx+num_samples])\n",
    "            \n",
    "        yield(\n",
    "            {\n",
    "                \"timeline\": timeline,\n",
    "                \"start_idx\": start_idx,\n",
    "                \"skip_odd\": skip_odd,\n",
    "                \"num_samples\": num_samples,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "60d346b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_generator(get_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f1a77b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timeline': [['LK'],\n",
       "  [],\n",
       "  ['LM'],\n",
       "  [],\n",
       "  ['LO'],\n",
       "  [],\n",
       "  ['LQ'],\n",
       "  [],\n",
       "  ['LS'],\n",
       "  [],\n",
       "  ['LU'],\n",
       "  [],\n",
       "  ['LW'],\n",
       "  [],\n",
       "  ['LY'],\n",
       "  [],\n",
       "  ['MA'],\n",
       "  [],\n",
       "  ['MC'],\n",
       "  [],\n",
       "  ['ME'],\n",
       "  [],\n",
       "  ['MG'],\n",
       "  [],\n",
       "  ['MI'],\n",
       "  [],\n",
       "  ['MK'],\n",
       "  [],\n",
       "  ['MM'],\n",
       "  [],\n",
       "  ['MO'],\n",
       "  [],\n",
       "  ['MQ'],\n",
       "  [],\n",
       "  ['MS'],\n",
       "  [],\n",
       "  ['MU'],\n",
       "  [],\n",
       "  ['MW'],\n",
       "  [],\n",
       "  ['MY'],\n",
       "  [],\n",
       "  ['NA'],\n",
       "  [],\n",
       "  ['NC'],\n",
       "  [],\n",
       "  ['NE'],\n",
       "  [],\n",
       "  ['NG'],\n",
       "  [],\n",
       "  ['NI'],\n",
       "  [],\n",
       "  ['NK'],\n",
       "  [],\n",
       "  ['NM'],\n",
       "  [],\n",
       "  ['NO'],\n",
       "  [],\n",
       "  ['NQ'],\n",
       "  []],\n",
       " 'start_idx': 296,\n",
       " 'skip_odd': True,\n",
       " 'num_samples': 1}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "428fcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_timeline_to_tokens(batched_samples: dict[str, list], separator: str)->dict[str, list]:\n",
    "    batched_samples[\"tokens\"] = [\n",
    "        [\n",
    "            timestep_value\n",
    "            for timestep in timeline\n",
    "            for timestep_value in [separator] + timestep\n",
    "        ]\n",
    "        for timeline in batched_samples[\"timeline\"]\n",
    "    ]\n",
    "    return batched_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e1c34301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e079fb9d3aee43a9836248c31704e4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda batch: batched_timeline_to_tokens(batch, SEPARATOR), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fd9dec95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deff0a027d5b46b782733670951f44fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd7eaf813c44766bd4bf4535cce3316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def batched_insert_static_feature_token(batched_samples: dict[str, list], key:str, insert_idx:int)->dict[str, list]:\n",
    "    for idx, _ in enumerate(batched_samples[\"tokens\"]):\n",
    "        batched_samples[\"tokens\"][idx].insert(insert_idx, str(batched_samples[key][idx]))\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "dataset = dataset.map(lambda batch: batched_insert_static_feature_token(batch, \"start_idx\", insert_idx=0), batched=True)\n",
    "dataset = dataset.map(lambda batch: batched_insert_static_feature_token(batch, \"skip_odd\", insert_idx=1), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6f20d868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['296', 'True', '<SEP>', 'LK', '<SEP>', '<SEP>', 'LM', '<SEP>', '<SEP>', 'LO']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"tokens\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3aa069",
   "metadata": {},
   "source": [
    "## Add position IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b07180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_add_position_ids(batched_samples: dict[str, list], separators:set[str])->dict[str, list]:\n",
    "    batched_samples[\"position_ids\"] = []\n",
    "    for tokens in batched_samples['stream']:\n",
    "        position_ids = []\n",
    "        cnt = 0\n",
    "        for token in tokens:\n",
    "            position_ids.append(cnt)\n",
    "            if token in separators:\n",
    "                cnt += 1\n",
    "        batched_samples[\"position_ids\"].append(position_ids)\n",
    "    return batched_samples\n",
    "\n",
    "dataset = dataset.map(lambda batch: batched_add_position_ids(batch, {SEPARATOR}), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        lambda examples: add_position_ids(examples, separators={'<SEP>', '<SEP-1>', '<SEP-7>' '<SEP-14>', '<SEP-30>', '<SEP-365>'}),\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf641980",
   "metadata": {},
   "source": [
    "# Get token_type2tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bef3781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_type2tokens = defaultdict(set)\n",
    "total_cnt = 0\n",
    "for _dataset in get_all_splits(dataset):\n",
    "    for stream in _dataset['stream']:\n",
    "        for example in stream:\n",
    "            token_type2tokens[example['token_type']].add(example['token'])\n",
    "            total_cnt += 1\n",
    "token_type2tokens = dict(token_type2tokens)\n",
    "pickle.dump(token_type2tokens, TOKEN_TYPES_PATH)\n",
    "fprint(\"Total number of annotations: \", total_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(token_type2tokens, TOKEN_TYPES_PATH)\n",
    "fprint(\"Total number of annotations: \", total_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c34003",
   "metadata": {},
   "source": [
    "# Cleanup stream and leave only what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5fbdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        lambda examples: cleanup_stream(examples, keep_time=True, keep_type=True, keep_position_ids=True,\n",
    "                                        keep_context_representation=False),\n",
    "        batched=True,\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad6184",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(JUST_BEFORE_ENCODING_DATASET_SPLIT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(JUST_BEFORE_ENCODING_DATASET_SPLIT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUST_BEFORE_ENCODING_DATASET_SPLIT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c09fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of patients fater intial filtering\n",
    "train_len = len(dataset['train'])\n",
    "test_len = len(dataset['test'])\n",
    "fprint(\"Total number of pts in train: \", train_len)\n",
    "fprint(\"Total number of pts in test: \", test_len)\n",
    "fprint(\"Total number of pts: \", train_len + test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bcc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of annotations per type after filtering\n",
    "cnt_per_type_after = {}\n",
    "for _dataset in get_all_splits(dataset):\n",
    "    for stream in _dataset['stream']:\n",
    "        for cui in stream:\n",
    "            if cat.cdb.cui2type_ids.get(cui, None):\n",
    "                t = list(cat.cdb.cui2type_ids[cui])[0]\n",
    "                cnt_per_type_after[t] = cnt_per_type_after.get(t, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fprint(\"Total number of annotations per type: \\n\")\n",
    "for t in cnt_per_type_after:\n",
    "    fprint(\"{:30}: {}\".format(cat.cdb.addl_info['type_id2name'][t].title(), cnt_per_type_after[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4d62e",
   "metadata": {},
   "source": [
    "# Make tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf2ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tokenizer = None\n",
    "#extra_tokenizer = SimpleMapTokenizer.load(\"./data/time/models/slam_tokenizer_annotations_stream_phase2_1d_200_ALL_TYPES.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e150f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_type2tokens = pickle.load(TOKEN_TYPES_PATH)\n",
    "extra_concepts = None\n",
    "if extra_tokenizer is not None:\n",
    "    extra_concepts = list(extra_tokenizer.tkn2id.keys())\n",
    "\n",
    "    for k,v in extra_tokenizer.token_type2tokens.items():\n",
    "        if k in token_type2tokens:\n",
    "            token_type2tokens[k].update(extra_tokenizer.token_type2tokens[k])\n",
    "        else:\n",
    "            token_type2tokens[k] = extra_tokenizer.token_type2tokens[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d8549",
   "metadata": {},
   "outputs": [],
   "source": [
    "_types = list(cdb.addl_info['type_id2name'].keys()) + list(token_type2tokens.keys())\n",
    "embeddings, tkn2id, id2tkn, = get_embeddings_for_tokens(dataset, cdb, context_type='xlong', types=_types,\n",
    "                                                        concepts=extra_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn2name = {tkn:cdb.get_name(tkn) for tkn in tkn2id.keys()}\n",
    "tokenizer = SimpleMapTokenizer(tkn2id=tkn2id, pad_id=tkn2id['<PAD>'], tkn2name=tkn2name,\n",
    "                               token_type2tokens=token_type2tokens, embeddings=embeddings,\n",
    "                               global_token_cnt=token_cnt, max_len=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b9cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokenizer.tkn2id) == len(tokenizer.id2tkn)\n",
    "assert len(tokenizer.embeddings) == len(tokenizer.id2tkn)\n",
    "assert len(tokenizer.tkn2name) == len(tokenizer.id2tkn)\n",
    "fprint(tokenizer.pad_id, tokenizer.id2tkn[tokenizer.pad_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.tkn2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0295fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "tokenizer.save(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of different concepts after all filtering\n",
    "fprint(\"Total number of concepts after filtering: \", len(tokenizer.tkn2id))\n",
    "fprint(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f996f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number annotations after all filtering\n",
    "fprint(\"Total number of annotations after filtering: \", sum([x for x in cnt_per_type_after.values()]))\n",
    "fprint(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862166a9",
   "metadata": {},
   "source": [
    "# Print number of different concepts per type after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebd72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_per_type = {}\n",
    "for cui in tkn2id:\n",
    "    if cat.cdb.cui2type_ids.get(cui, ['Other']):\n",
    "        t = list(cat.cdb.cui2type_ids.get(cui, ['Other']))[0]\n",
    "        cnt_per_type[t] = cnt_per_type.get(t, 0) + 1\n",
    "fprint(\"Total number of <<different>> concepts per type after filtering\")\n",
    "for t in cnt_per_type:\n",
    "    fprint(\"{:30}: {}\".format(cat.cdb.addl_info['type_id2name'].get(t, t).title(), cnt_per_type[t]))\n",
    "fprint(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf388d",
   "metadata": {},
   "source": [
    "# Create global tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "_types = list(cdb.addl_info['type_id2name'].keys()) + list(token_type2tokens.keys())\n",
    "concepts = list(cat.config.linking['filters']['cuis'])\n",
    "embeddings, tkn2id, id2tkn, = get_embeddings_for_tokens(dataset, cdb, context_type='xlong', types=_types, concepts=concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabbbf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn2name = {tkn:cdb.get_name(tkn) for tkn in tkn2id.keys()}\n",
    "tokenizer = SimpleMapTokenizer(tkn2id=tkn2id, pad_id=tkn2id['<PAD>'], tkn2name=tkn2name,\n",
    "                               token_type2tokens=token_type2tokens, embeddings=embeddings,\n",
    "                               global_token_cnt=token_cnt, max_len=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22833eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(BASE_TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06d09b",
   "metadata": {},
   "source": [
    "# Convert tokens to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864df946",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FROM_BASE:\n",
    "    print(\"USING BASE TOKENIZER\")\n",
    "    TOKENIZER_PATH = BASE_TOKENIZER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4540c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  SimpleMapTokenizer.load(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "        lambda examples: tokenizer.encode(examples),\n",
    "        batched=True,\n",
    "        remove_columns=['stream'],\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8718ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.save_to_disk(PREPARED_DATASET_SPLIT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dd11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPARED_DATASET_SPLIT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6715bb47",
   "metadata": {},
   "source": [
    "# Test is all OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60881f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = datasets.load_from_disk(PREPARED_DATASET_SPLIT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(JUST_BEFORE_ENCODING_DATASET_SPLIT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleMapTokenizer.load(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d66a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 1096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664dd895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b1cbc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[cdb.get_name(x) for x in dataset['train'][ind]['stream']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0594e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ty, p, t, c in zip(encoded_dataset['train'][ind]['token_type'], encoded_dataset['train'][ind]['position_ids'], encoded_dataset['train'][ind]['time'], tokenizer.convert_ids2tokens(encoded_dataset['train'][ind]['input_ids'])):\n",
    "    print(datetime.fromtimestamp(t), p, \"{:20}\".format(ty), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19003ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset['train'][ind]['patient_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a3cc65",
   "metadata": {},
   "source": [
    "# Preapre for Foresight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 32330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55457df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[cdb.get_name(x) for x in dataset['train'][ind]['stream']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196eaf9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, c in enumerate(dataset['train'][ind]['stream']):\n",
    "    print(i)\n",
    "    if i > 20 and c not in dataset['train'][ind]['stream'][0:i]:\n",
    "        print(i, c, cdb.get_name(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a159ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for i, cui in enumerate(dataset['train'][ind]['stream'][:161]):\n",
    "    d = {\n",
    "        'id': cui,\n",
    "        'label': cdb.get_name(cui),\n",
    "        'count': 1000000,\n",
    "        'name': cdb.get_name(cui),\n",
    "        'cui': cui,\n",
    "        'saliency': 0,\n",
    "        'uid': i\n",
    "    }\n",
    "    out.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f40a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(out, open(\"./data/tmp/timeline_example_1.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2730480",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

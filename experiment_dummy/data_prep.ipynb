{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ae6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import datasets\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "# from foresight.datasets import patient_concept_stream\n",
    "# from foresight.datasets.filters import filter_by_count, filter_by_type\n",
    "# from foresight.datasets.utils import get_embeddings_for_tokens, stream_to_separate_examples, add_to_stream, \\\n",
    "#                                   remove_parents_from_stream, bucket_concepts, cleanup_stream, \\\n",
    "#                                   split_stream, add_age, get_all_splits, add_ttd, add_position_ids\n",
    "# from foresight.utils import pickle\n",
    "# from foresight.utils.cdb_utils import get_parents_map \n",
    "# from foresight.utils.stream_utils import docs2stream, calculate_counts\n",
    "# from foresight.tokenizers.simple_map_tokenizer import SimpleMapTokenizer\n",
    "# from foresight.metrics.next_concept_prediction import precision, metrics_data2df, ComputePrecisionHF\n",
    "# from foresight.utils import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c38222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import Random\n",
    "from foresight.tokenizers.my_map_tokenizer import MapTokenizer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c930ab",
   "metadata": {},
   "source": [
    "## Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b27dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TIMELINES = 1000\n",
    "LETTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "NUM_TIMESTEPS = 10\n",
    "RANDOM_SEED = Random(23)\n",
    "MAX_SKIP = 10\n",
    "MAX_NUM_SAMPLES = 3\n",
    "\n",
    "SEQUENCE_LENGTH = 12\n",
    "SEPARATOR_TOKEN = \"<SEP>\"\n",
    "PADDING_TOKEN = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0faa734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    for _ in range(1000):\n",
    "        start_idx = RANDOM_SEED.randint(0, (len(LETTERS)/2))\n",
    "        skip_odd = RANDOM_SEED.choice([True, False])\n",
    "        num_samples = RANDOM_SEED.randint(1, MAX_NUM_SAMPLES)\n",
    "\n",
    "        timeline = []\n",
    "        for seq_idx in range(SEQUENCE_LENGTH):\n",
    "            char_idx = start_idx + num_samples*seq_idx\n",
    "            if skip_odd and char_idx % 2 == 1:\n",
    "                timeline.append([])\n",
    "            elif char_idx+num_samples >= len(LETTERS):\n",
    "                timeline.append([])\n",
    "            else:\n",
    "                timeline.append(list(LETTERS[char_idx:char_idx+num_samples]))\n",
    "            \n",
    "        yield(\n",
    "            {\n",
    "                \"timeline\": timeline,\n",
    "                \"start_idx\": start_idx,\n",
    "                \"skip_odd\": skip_odd,\n",
    "                \"num_samples\": num_samples,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d346b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_generator(get_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1a77b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timeline': [['M'],\n",
       "  ['N'],\n",
       "  ['O'],\n",
       "  ['P'],\n",
       "  ['Q'],\n",
       "  ['R'],\n",
       "  ['S'],\n",
       "  ['T'],\n",
       "  ['U'],\n",
       "  ['V'],\n",
       "  ['W'],\n",
       "  ['X']],\n",
       " 'start_idx': 12,\n",
       " 'skip_odd': False,\n",
       " 'num_samples': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "428fcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_timeline_to_tokens(batched_samples: dict[str, list], separator: str)->dict[str, list]:\n",
    "    batched_samples[\"tokens\"] = [\n",
    "        [\n",
    "            timestep_value\n",
    "            for timestep in timeline\n",
    "            for timestep_value in [separator] + timestep\n",
    "        ]\n",
    "        for timeline in batched_samples[\"timeline\"]\n",
    "    ]\n",
    "    return batched_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c34301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['M'], ['N'], ['O'], ['P'], ['Q'], ['R'], ['S'], ['T'], ['U'], ['V']],\n",
       " ['<SEP>', 'M', '<SEP>', 'N', '<SEP>', 'O', '<SEP>', 'P', '<SEP>', 'Q'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda batch: batched_timeline_to_tokens(batch, SEPARATOR_TOKEN), batched=True)\n",
    "dataset[0][\"timeline\"][:10], dataset[0][\"tokens\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd9dec95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start_idx_12',\n",
       " 'skip_odd_False',\n",
       " 'num_samples_1',\n",
       " '<SEP>',\n",
       " 'M',\n",
       " '<SEP>',\n",
       " 'N',\n",
       " '<SEP>',\n",
       " 'O',\n",
       " '<SEP>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_insert_static_feature_token(batched_samples: dict[str, list], key:str, insert_idx:int)->dict[str, list]:\n",
    "    for idx, _ in enumerate(batched_samples[\"tokens\"]):\n",
    "        batched_samples[\"tokens\"][idx].insert(insert_idx, f\"{key}_{batched_samples[key][idx]}\")\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "dataset = dataset.map(lambda batch: batched_insert_static_feature_token(batch, \"start_idx\", insert_idx=0), batched=True)\n",
    "dataset = dataset.map(lambda batch: batched_insert_static_feature_token(batch, \"skip_odd\", insert_idx=1), batched=True)\n",
    "dataset = dataset.map(lambda batch: batched_insert_static_feature_token(batch, \"num_samples\", insert_idx=2), batched=True)\n",
    "dataset[0][\"tokens\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3aa069",
   "metadata": {},
   "source": [
    "## Add position IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b07180d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('start_idx_12', 0),\n",
       " ('skip_odd_False', 0),\n",
       " ('num_samples_1', 0),\n",
       " ('<SEP>', 1),\n",
       " ('M', 1),\n",
       " ('<SEP>', 2),\n",
       " ('N', 2),\n",
       " ('<SEP>', 3),\n",
       " ('O', 3),\n",
       " ('<SEP>', 4)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_add_position_ids(batched_samples: dict[str, list], separators:set[str])->dict[str, list]:\n",
    "    batched_samples[\"position_ids\"] = []\n",
    "    for tokens in batched_samples['tokens']:\n",
    "        position_ids = []\n",
    "        cnt = 0\n",
    "        for token in tokens:\n",
    "            if token in separators:\n",
    "                cnt += 1\n",
    "            position_ids.append(cnt)\n",
    "        batched_samples[\"position_ids\"].append(position_ids)\n",
    "    return batched_samples\n",
    "\n",
    "dataset = dataset.map(lambda batch: batched_add_position_ids(batch, {SEPARATOR_TOKEN}), batched=True)\n",
    "[(dataset[0][\"tokens\"][idx], dataset[0][\"position_ids\"][idx]) for idx in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7792d191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "len(dataset[\"train\"]), len(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4d62e",
   "metadata": {},
   "source": [
    "# Make tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "371f82cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'<SEP>': 9600,\n",
       "         'O': 642,\n",
       "         'M': 634,\n",
       "         'Q': 590,\n",
       "         'S': 580,\n",
       "         'U': 542,\n",
       "         'K': 541,\n",
       "         'N': 504,\n",
       "         'W': 481,\n",
       "         'P': 467,\n",
       "         'T': 452,\n",
       "         'R': 449,\n",
       "         'I': 442,\n",
       "         'L': 432,\n",
       "         'V': 419,\n",
       "         'skip_odd_False': 402,\n",
       "         'skip_odd_True': 398,\n",
       "         'X': 369,\n",
       "         'J': 356,\n",
       "         'G': 336,\n",
       "         'H': 291,\n",
       "         'num_samples_3': 270,\n",
       "         'num_samples_2': 265,\n",
       "         'num_samples_1': 265,\n",
       "         'E': 240,\n",
       "         'F': 210,\n",
       "         'Y': 181,\n",
       "         'C': 154,\n",
       "         'D': 133,\n",
       "         'B': 80,\n",
       "         'start_idx_10': 73,\n",
       "         'start_idx_0': 67,\n",
       "         'A': 67,\n",
       "         'start_idx_12': 63,\n",
       "         'start_idx_9': 62,\n",
       "         'start_idx_8': 58,\n",
       "         'start_idx_5': 57,\n",
       "         'start_idx_2': 56,\n",
       "         'start_idx_4': 56,\n",
       "         'start_idx_3': 55,\n",
       "         'start_idx_11': 54,\n",
       "         'start_idx_7': 54,\n",
       "         'start_idx_13': 51,\n",
       "         'start_idx_1': 47,\n",
       "         'start_idx_6': 47})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token_count = Counter(token for tokens in dataset[\"train\"][\"tokens\"] for token in tokens)\n",
    "token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78cb8ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_idx_2': 0,\n",
       " 'skip_odd_True': 1,\n",
       " 'num_samples_2': 2,\n",
       " '<SEP>': 3,\n",
       " 'C': 4,\n",
       " 'D': 5,\n",
       " 'E': 6,\n",
       " 'F': 7,\n",
       " 'G': 8,\n",
       " 'H': 9,\n",
       " 'I': 10,\n",
       " 'J': 11,\n",
       " 'K': 12,\n",
       " 'L': 13,\n",
       " 'M': 14,\n",
       " 'N': 15,\n",
       " 'O': 16,\n",
       " 'P': 17,\n",
       " 'Q': 18,\n",
       " 'R': 19,\n",
       " 'S': 20,\n",
       " 'T': 21,\n",
       " 'U': 22,\n",
       " 'V': 23,\n",
       " 'W': 24,\n",
       " 'X': 25,\n",
       " 'start_idx_10': 26,\n",
       " 'skip_odd_False': 27,\n",
       " 'num_samples_3': 28,\n",
       " 'Y': 29,\n",
       " 'start_idx_11': 30,\n",
       " 'start_idx_4': 31,\n",
       " 'start_idx_8': 32,\n",
       " 'start_idx_7': 33,\n",
       " 'start_idx_1': 34,\n",
       " 'start_idx_12': 35,\n",
       " 'num_samples_1': 36,\n",
       " 'start_idx_5': 37,\n",
       " 'B': 38,\n",
       " 'start_idx_9': 39,\n",
       " 'start_idx_13': 40,\n",
       " 'start_idx_6': 41,\n",
       " 'start_idx_0': 42,\n",
       " 'A': 43,\n",
       " 'start_idx_3': 44}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkn_to_id = {tkn: idx for idx, tkn in enumerate(token_count.keys())}\n",
    "tkn_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebeda7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MapTokenizer(tkn_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "507b6e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 1, 2, 3, 4, 5, 3, 6, 7, 3, 8, 9, 3, 10, 11, 3, 12, 13, 3, 14, 15, 3, 16, 17, 3, 18, 19, 3, 20, 21, 3, 22, 23, 3, 24, 25, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer(dataset[\"train\"][\"tokens\"][0], is_split_into_words=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8741d5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(ids[\"input_ids\"]) == dataset[\"train\"][\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a70e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54691592",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(dataset[\"train\"][\"tokens\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "AutoTokenizer.from_pretrained(\"bert-base-cased\").encode_plus([\"hello\", \"world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(dataset[\"train\"][\"tokens\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a42997",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleMapTokenizer(tkn2id=tkn2id, pad_id=tkn2id['<PAD>'], tkn2name=tkn2name,\n",
    "                               token_type2tokens=token_type2tokens, embeddings=embeddings,\n",
    "                               global_token_cnt=token_cnt, max_len=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d8549",
   "metadata": {},
   "outputs": [],
   "source": [
    "_types = list(cdb.addl_info['type_id2name'].keys()) + list(token_type2tokens.keys())\n",
    "embeddings, tkn2id, id2tkn, = get_embeddings_for_tokens(dataset, cdb, context_type='xlong', types=_types,\n",
    "                                                        concepts=extra_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn2name = {tkn:cdb.get_name(tkn) for tkn in tkn2id.keys()}\n",
    "tokenizer = SimpleMapTokenizer(tkn2id=tkn2id, pad_id=tkn2id['<PAD>'], tkn2name=tkn2name,\n",
    "                               token_type2tokens=token_type2tokens, embeddings=embeddings,\n",
    "                               global_token_cnt=token_cnt, max_len=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b9cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokenizer.tkn2id) == len(tokenizer.id2tkn)\n",
    "assert len(tokenizer.embeddings) == len(tokenizer.id2tkn)\n",
    "assert len(tokenizer.tkn2name) == len(tokenizer.id2tkn)\n",
    "fprint(tokenizer.pad_id, tokenizer.id2tkn[tokenizer.pad_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.tkn2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0295fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "tokenizer.save(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of different concepts after all filtering\n",
    "fprint(\"Total number of concepts after filtering: \", len(tokenizer.tkn2id))\n",
    "fprint(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f996f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number annotations after all filtering\n",
    "fprint(\"Total number of annotations after filtering: \", sum([x for x in cnt_per_type_after.values()]))\n",
    "fprint(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862166a9",
   "metadata": {},
   "source": [
    "# Print number of different concepts per type after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebd72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_per_type = {}\n",
    "for cui in tkn2id:\n",
    "    if cat.cdb.cui2type_ids.get(cui, ['Other']):\n",
    "        t = list(cat.cdb.cui2type_ids.get(cui, ['Other']))[0]\n",
    "        cnt_per_type[t] = cnt_per_type.get(t, 0) + 1\n",
    "fprint(\"Total number of <<different>> concepts per type after filtering\")\n",
    "for t in cnt_per_type:\n",
    "    fprint(\"{:30}: {}\".format(cat.cdb.addl_info['type_id2name'].get(t, t).title(), cnt_per_type[t]))\n",
    "fprint(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf388d",
   "metadata": {},
   "source": [
    "# Create global tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "_types = list(cdb.addl_info['type_id2name'].keys()) + list(token_type2tokens.keys())\n",
    "concepts = list(cat.config.linking['filters']['cuis'])\n",
    "embeddings, tkn2id, id2tkn, = get_embeddings_for_tokens(dataset, cdb, context_type='xlong', types=_types, concepts=concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabbbf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn2name = {tkn:cdb.get_name(tkn) for tkn in tkn2id.keys()}\n",
    "tokenizer = SimpleMapTokenizer(tkn2id=tkn2id, pad_id=tkn2id['<PAD>'], tkn2name=tkn2name,\n",
    "                               token_type2tokens=token_type2tokens, embeddings=embeddings,\n",
    "                               global_token_cnt=token_cnt, max_len=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22833eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(BASE_TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06d09b",
   "metadata": {},
   "source": [
    "# Convert tokens to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864df946",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FROM_BASE:\n",
    "    print(\"USING BASE TOKENIZER\")\n",
    "    TOKENIZER_PATH = BASE_TOKENIZER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4540c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  SimpleMapTokenizer.load(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "        lambda examples: tokenizer.encode(examples),\n",
    "        batched=True,\n",
    "        remove_columns=['stream'],\n",
    "        load_from_cache_file=False,\n",
    "        num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8718ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.save_to_disk(PREPARED_DATASET_SPLIT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dd11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPARED_DATASET_SPLIT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6715bb47",
   "metadata": {},
   "source": [
    "# Test is all OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60881f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = datasets.load_from_disk(PREPARED_DATASET_SPLIT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(JUST_BEFORE_ENCODING_DATASET_SPLIT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleMapTokenizer.load(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d66a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 1096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664dd895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b1cbc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[cdb.get_name(x) for x in dataset['train'][ind]['stream']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0594e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ty, p, t, c in zip(encoded_dataset['train'][ind]['token_type'], encoded_dataset['train'][ind]['position_ids'], encoded_dataset['train'][ind]['time'], tokenizer.convert_ids2tokens(encoded_dataset['train'][ind]['input_ids'])):\n",
    "    print(datetime.fromtimestamp(t), p, \"{:20}\".format(ty), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19003ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset['train'][ind]['patient_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a3cc65",
   "metadata": {},
   "source": [
    "# Preapre for Foresight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 32330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55457df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[cdb.get_name(x) for x in dataset['train'][ind]['stream']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196eaf9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, c in enumerate(dataset['train'][ind]['stream']):\n",
    "    print(i)\n",
    "    if i > 20 and c not in dataset['train'][ind]['stream'][0:i]:\n",
    "        print(i, c, cdb.get_name(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a159ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for i, cui in enumerate(dataset['train'][ind]['stream'][:161]):\n",
    "    d = {\n",
    "        'id': cui,\n",
    "        'label': cdb.get_name(cui),\n",
    "        'count': 1000000,\n",
    "        'name': cdb.get_name(cui),\n",
    "        'cui': cui,\n",
    "        'saliency': 0,\n",
    "        'uid': i\n",
    "    }\n",
    "    out.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f40a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(out, open(\"./data/tmp/timeline_example_1.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2730480",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

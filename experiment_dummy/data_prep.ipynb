{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import floor\n",
    "from pathlib import Path\n",
    "from random import Random\n",
    "from typing import Any, Iterator\n",
    "\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer, models, trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foresight.tokenizers import PreTrainedTokenizerFastWithPositionIDPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c930ab",
   "metadata": {},
   "source": [
    "## Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TIMELINES = 10000\n",
    "LETTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "RANDOM_SEED = Random(23)\n",
    "MAX_NUM_SAMPLES = 3\n",
    "\n",
    "SEPARATOR_TOKEN = \"<SEP>\"\n",
    "PADDING_TOKEN = \"<PAD>\"\n",
    "UNKNOWN_TOKEN = \"<UNK>\"\n",
    "\n",
    "OUTPUT_DIR = Path.cwd() / \"outputs\"\n",
    "SAVE_TOKENIZER_PATH = OUTPUT_DIR / \"tokenizer\"\n",
    "SAVE_ENCODED_DATASET_PATH = OUTPUT_DIR / \"encoded_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(num_timelines: int) -> Iterator[dict[str, Any]]:\n",
    "    for _ in range(num_timelines):\n",
    "        start_idx = RANDOM_SEED.randint(0, floor(len(LETTERS) / 2))\n",
    "        skip_odd = RANDOM_SEED.choice([True, False])\n",
    "        num_samples = RANDOM_SEED.randint(1, MAX_NUM_SAMPLES)\n",
    "\n",
    "        timeline: list[list[str]] = []\n",
    "        for char_idx in range(start_idx, len(LETTERS), num_samples):\n",
    "            if skip_odd and char_idx % 2 == 1:\n",
    "                timeline.append([])\n",
    "            else:\n",
    "                timeline.append(list(LETTERS[char_idx : char_idx + num_samples]))\n",
    "\n",
    "        yield (\n",
    "            {\n",
    "                \"timeline\": timeline,\n",
    "                \"start_idx\": start_idx,\n",
    "                \"skip_odd\": skip_odd,\n",
    "                \"num_samples\": num_samples,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d346b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_generator(lambda: get_samples(NUM_TIMELINES))\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_timeline_to_tokens(\n",
    "    batched_samples: dict[str, list], separator: str\n",
    ") -> dict[str, list]:\n",
    "    batched_samples[\"tokens\"] = [\n",
    "        [\n",
    "            timestep_value\n",
    "            for timestep in timeline\n",
    "            for timestep_value in [separator] + timestep\n",
    "        ]\n",
    "        for timeline in batched_samples[\"timeline\"]\n",
    "    ]\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_timeline_to_tokens(batch, SEPARATOR_TOKEN), batched=True\n",
    ")\n",
    "dataset[0][\"timeline\"][:10], dataset[0][\"tokens\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9dec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_insert_static_feature_token(\n",
    "    batched_samples: dict[str, list], key: str, insert_idx: int\n",
    ") -> dict[str, list]:\n",
    "    for idx, _ in enumerate(batched_samples[\"tokens\"]):\n",
    "        batched_samples[\"tokens\"][idx].insert(\n",
    "            insert_idx, f\"{key}_{batched_samples[key][idx]}\"\n",
    "        )\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_insert_static_feature_token(batch, \"start_idx\", insert_idx=0),\n",
    "    batched=True,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_insert_static_feature_token(batch, \"skip_odd\", insert_idx=1),\n",
    "    batched=True,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_insert_static_feature_token(\n",
    "        batch, \"num_samples\", insert_idx=2\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "dataset[0][\"tokens\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3aa069",
   "metadata": {},
   "source": [
    "## Add position IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b07180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_add_position_ids(\n",
    "    batched_samples: dict[str, list], separators: set[str]\n",
    ") -> dict[str, list]:\n",
    "    batched_samples[\"position_ids\"] = []\n",
    "    for tokens in batched_samples[\"tokens\"]:\n",
    "        position_ids = []\n",
    "        cnt = 0\n",
    "        for token in tokens:\n",
    "            if token in separators:\n",
    "                cnt += 1\n",
    "            position_ids.append(cnt)\n",
    "        batched_samples[\"position_ids\"].append(position_ids)\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_add_position_ids(batch, {SEPARATOR_TOKEN}), batched=True\n",
    ")\n",
    "print(list(zip(dataset[0][\"tokens\"], dataset[0][\"position_ids\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4d62e",
   "metadata": {},
   "source": [
    "# Make tokenizer\n",
    "\n",
    "Adapted from https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt#building-a-wordpiece-tokenizer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = Counter(\n",
    "    token for tokens in dataset[\"train\"][\"tokens\"] for token in tokens\n",
    ")\n",
    "print(token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordLevel(unk_token=UNKNOWN_TOKEN))\n",
    "# Separator token is already in the dataset\n",
    "trainer = trainers.WordLevelTrainer(special_tokens=[UNKNOWN_TOKEN, PADDING_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48dd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(dataset[\"train\"][\"tokens\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(dataset[\"train\"][0][\"tokens\"], is_pretokenized=True)\n",
    "print(list(zip(encoding.tokens, encoding.ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe15b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_fast_tokenizer = PreTrainedTokenizerFastWithPositionIDPadding(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=UNKNOWN_TOKEN,\n",
    "    pad_token=PADDING_TOKEN,\n",
    "    sep_token=SEPARATOR_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sample = pretrained_fast_tokenizer(\n",
    "    dataset[\"train\"][0][\"tokens\"], is_split_into_words=True\n",
    ")\n",
    "encoded_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_fast_tokenizer.save_pretrained(SAVE_TOKENIZER_PATH)\n",
    "reloaded_tokenizer = PreTrainedTokenizerFastWithPositionIDPadding.from_pretrained(\n",
    "    SAVE_TOKENIZER_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6cf3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sample_reloaded = reloaded_tokenizer(\n",
    "    dataset[\"train\"][0][\"tokens\"], is_split_into_words=True\n",
    ")\n",
    "assert encoded_sample == encoded_sample_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "    lambda batch: pretrained_fast_tokenizer(\n",
    "        batch[\"tokens\"], is_split_into_words=True, return_token_type_ids=False\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=[\"timeline\", \"start_idx\", \"skip_odd\", \"num_samples\", \"tokens\"],\n",
    ")\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ed6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in encoded_dataset[\"train\"][0].items():\n",
    "    print(key, value[:10] if type(value) == list else value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb1c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.save_to_disk(SAVE_ENCODED_DATASET_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

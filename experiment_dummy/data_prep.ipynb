{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import floor\n",
    "from pathlib import Path\n",
    "from random import Random\n",
    "from typing import Any, Iterator\n",
    "\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer, models, trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foresight.tokenizers import PreTrainedTokenizerFastWithPositionIDPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c930ab",
   "metadata": {},
   "source": [
    "## Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TIMELINES = 10000\n",
    "LETTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "RANDOM_SEED = Random(23)\n",
    "MAX_NUM_SAMPLES = 3\n",
    "\n",
    "SEPARATOR_TOKEN = \"<SEP>\"\n",
    "PADDING_TOKEN = \"<PAD>\"\n",
    "UNKNOWN_TOKEN = \"<UNK>\"\n",
    "\n",
    "OUTPUT_DIR = Path.cwd() / \"outputs\"\n",
    "SAVE_TOKENIZER_PATH = OUTPUT_DIR / \"tokenizer\"\n",
    "SAVE_ENCODED_DATASET_PATH = OUTPUT_DIR / \"encoded_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(num_timelines: int) -> Iterator[dict[str, Any]]:\n",
    "    for _ in range(num_timelines):\n",
    "        start_idx = RANDOM_SEED.randint(0, floor(len(LETTERS) / 2))\n",
    "        num_skips = RANDOM_SEED.randint(0, 2)\n",
    "        num_samples = RANDOM_SEED.randint(1, 3)\n",
    "\n",
    "        timeline: list[list[str]] = []\n",
    "        char_diffs: list[int] = []\n",
    "        start_char_idx = start_idx\n",
    "        while start_char_idx < len(LETTERS):\n",
    "            end_char_idx = min(start_char_idx + num_samples, len(LETTERS))\n",
    "            timeline.append(list(LETTERS[start_char_idx:end_char_idx]))\n",
    "            char_diffs.append(num_skips)\n",
    "\n",
    "            start_char_idx = end_char_idx + num_skips\n",
    "\n",
    "        yield (\n",
    "            {\n",
    "                \"timeline\": timeline,\n",
    "                \"char_diffs\": char_diffs,\n",
    "                \"start_idx\": start_idx,\n",
    "                \"num_blanks\": num_skips,\n",
    "                \"num_samples\": num_samples,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d346b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_generator(lambda: get_samples(NUM_TIMELINES))\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_timeline_to_tokens(\n",
    "    batched_samples: dict[str, list], separator: str\n",
    ") -> dict[str, list]:\n",
    "    batched_samples[\"tokens\"] = [\n",
    "        [\n",
    "            token\n",
    "            for timestep, char_diff in zip(timeline, char_diffs)\n",
    "            for token in [f\"char_diff_{char_diff}\"] + timestep + [separator]\n",
    "        ]\n",
    "        for timeline, char_diffs in zip(\n",
    "            batched_samples[\"timeline\"], batched_samples[\"char_diffs\"]\n",
    "        )\n",
    "    ]\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_timeline_to_tokens(batch, SEPARATOR_TOKEN), batched=True\n",
    ")\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9dec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_prepend_token(\n",
    "    batched_samples: dict[str, list], token: str\n",
    ") -> dict[str, list]:\n",
    "    for idx, _ in enumerate(batched_samples[\"tokens\"]):\n",
    "        batched_samples[\"tokens\"][idx].insert(0, token)\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "def batched_prepend_static_feature_token(\n",
    "    batched_samples: dict[str, list], key: str\n",
    ") -> dict[str, list]:\n",
    "    for idx, _ in enumerate(batched_samples[\"tokens\"]):\n",
    "        batched_samples[\"tokens\"][idx].insert(0, f\"{key}_{batched_samples[key][idx]}\")\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_prepend_token(batch, SEPARATOR_TOKEN),\n",
    "    batched=True,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_prepend_static_feature_token(batch, \"start_idx\"),\n",
    "    batched=True,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_prepend_static_feature_token(batch, \"num_blanks\"),\n",
    "    batched=True,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_prepend_static_feature_token(batch, \"num_samples\"),\n",
    "    batched=True,\n",
    ")\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4d62e",
   "metadata": {},
   "source": [
    "# Make tokenizer\n",
    "\n",
    "Adapted from https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt#building-a-wordpiece-tokenizer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = Counter(\n",
    "    token for tokens in dataset[\"train\"][\"tokens\"] for token in tokens\n",
    ")\n",
    "print(token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordLevel(unk_token=UNKNOWN_TOKEN))\n",
    "# Separator token is already in the dataset\n",
    "trainer = trainers.WordLevelTrainer(special_tokens=[UNKNOWN_TOKEN, PADDING_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48dd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(dataset[\"train\"][\"tokens\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(dataset[\"train\"][0][\"tokens\"], is_pretokenized=True)\n",
    "print(list(zip(encoding.tokens, encoding.ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe15b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_fast_tokenizer = PreTrainedTokenizerFastWithPositionIDPadding(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=UNKNOWN_TOKEN,\n",
    "    pad_token=PADDING_TOKEN,\n",
    "    sep_token=SEPARATOR_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sample = pretrained_fast_tokenizer(\n",
    "    dataset[\"train\"][0][\"tokens\"], is_split_into_words=True\n",
    ")\n",
    "encoded_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_fast_tokenizer.save_pretrained(SAVE_TOKENIZER_PATH)\n",
    "reloaded_tokenizer = PreTrainedTokenizerFastWithPositionIDPadding.from_pretrained(\n",
    "    SAVE_TOKENIZER_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6cf3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sample_reloaded = reloaded_tokenizer(\n",
    "    dataset[\"train\"][0][\"tokens\"], is_split_into_words=True\n",
    ")\n",
    "assert encoded_sample == encoded_sample_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48659ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "    lambda batch: pretrained_fast_tokenizer(\n",
    "        batch[\"tokens\"], is_split_into_words=True, return_token_type_ids=False\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "        \"timeline\",\n",
    "        \"char_diffs\",\n",
    "        \"start_idx\",\n",
    "        \"num_blanks\",\n",
    "        \"num_samples\",\n",
    "        \"tokens\",\n",
    "    ],\n",
    ")\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ed6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in encoded_dataset[\"train\"][0].items():\n",
    "    print(key, value[:10] if type(value) == list else value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb1c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.save_to_disk(SAVE_ENCODED_DATASET_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "from random import Random\n",
    "from typing import Any, Iterator\n",
    "\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer, models, trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foresight.tokenizers import PreTrainedTokenizerFastWithPositionIDPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c930ab",
   "metadata": {},
   "source": [
    "## Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TIMELINES = 1000\n",
    "YEAR_CUTOFF = 2024\n",
    "VAL_YEAR_CUTOFF = 2020\n",
    "CONDITIONS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "RANDOM_SEED = Random(23)\n",
    "MAX_NUM_SAMPLES = 3\n",
    "\n",
    "SEPARATOR_TOKEN = \"<SEP>\"\n",
    "PADDING_TOKEN = \"<PAD>\"\n",
    "UNKNOWN_TOKEN = \"<UNK>\"\n",
    "EOS_TOKEN = \"Z\"\n",
    "\n",
    "OUTPUT_DIR = Path.cwd() / \"outputs\"\n",
    "SAVE_TOKENIZER_PATH = OUTPUT_DIR / \"tokenizer\"\n",
    "SAVE_ENCODED_DATASET_PATH = OUTPUT_DIR / \"encoded_dataset\"\n",
    "SAVE_RAW_DATASET_PATH = OUTPUT_DIR / \"raw_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(num_timelines: int, year_cutoff: int) -> Iterator[dict[str, Any]]:\n",
    "    for _ in range(num_timelines):\n",
    "        year_of_birth = RANDOM_SEED.randint(2005, 2010)\n",
    "        sex = RANDOM_SEED.choice([\"M\", \"F\"])\n",
    "        ethnicity = RANDOM_SEED.choice([\"ETH_1\", \"ETH_2\"])\n",
    "\n",
    "        time_step = 1 if sex == \"M\" else 2\n",
    "        num_samples = 1 if ethnicity == \"ETH_1\" else 2 if ethnicity == \"ETH_2\" else 3\n",
    "        start_condition_idx = RANDOM_SEED.randint(0, len(CONDITIONS) - 1)\n",
    "        timestamp = year_of_birth + RANDOM_SEED.randint(0, 10)\n",
    "\n",
    "        timeline: list[list[str]] = []\n",
    "        timestamps: list[int] = []\n",
    "\n",
    "        while timestamp < year_cutoff and start_condition_idx < len(CONDITIONS):\n",
    "            end_condition_idx = min(start_condition_idx + num_samples, len(CONDITIONS))\n",
    "            timeline.append(list(CONDITIONS[start_condition_idx:end_condition_idx]))\n",
    "            timestamps.append(timestamp)\n",
    "\n",
    "            start_condition_idx = end_condition_idx\n",
    "            timestamp += time_step\n",
    "\n",
    "        yield (\n",
    "            {\n",
    "                \"timeline\": timeline,\n",
    "                \"timestamps\": timestamps,\n",
    "                \"year_of_birth\": year_of_birth,\n",
    "                \"sex\": sex,\n",
    "                \"ethnicity\": ethnicity,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d346b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_generator(lambda: get_samples(NUM_TIMELINES, YEAR_CUTOFF))\n",
    "for data in islice(dataset, 5):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_timeline_to_train_tokens(\n",
    "    batched_samples: dict[str, list], separator: str, val_year_cutoff: int\n",
    ") -> dict[str, list]:\n",
    "    batched_time_diffs = [\n",
    "        [t1 - t0 for t0, t1 in zip([year_of_birth] + timestamps, timestamps)]\n",
    "        for year_of_birth, timestamps in zip(\n",
    "            batched_samples[\"year_of_birth\"], batched_samples[\"timestamps\"]\n",
    "        )\n",
    "    ]\n",
    "    batched_samples[\"tokens\"] = [\n",
    "        [\n",
    "            token\n",
    "            for condition, timestamp, time_diff in zip(timeline, timestamps, time_diffs)\n",
    "            if timestamp < val_year_cutoff\n",
    "            for token in [f\"time_diff_{time_diff}\"] + condition + [separator]\n",
    "        ]\n",
    "        for timeline, timestamps, time_diffs in zip(\n",
    "            batched_samples[\"timeline\"],\n",
    "            batched_samples[\"timestamps\"],\n",
    "            batched_time_diffs,\n",
    "        )\n",
    "    ]\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_timeline_to_train_tokens(\n",
    "        batch, SEPARATOR_TOKEN, VAL_YEAR_CUTOFF\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "for data in islice(dataset, 5):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9dec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_prepend_token(\n",
    "    batched_samples: dict[str, list], token: str\n",
    ") -> dict[str, list]:\n",
    "    for idx, _ in enumerate(batched_samples[\"tokens\"]):\n",
    "        batched_samples[\"tokens\"][idx].insert(0, token)\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "def batched_prepend_static_feature_token(\n",
    "    batched_samples: dict[str, list], key: str\n",
    ") -> dict[str, list]:\n",
    "    for idx, _ in enumerate(batched_samples[\"tokens\"]):\n",
    "        batched_samples[\"tokens\"][idx].insert(0, f\"{key}_{batched_samples[key][idx]}\")\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_prepend_token(batch, SEPARATOR_TOKEN),\n",
    "    batched=True,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_prepend_static_feature_token(batch, \"year_of_birth\"),\n",
    "    batched=True,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_prepend_static_feature_token(batch, \"sex\"),\n",
    "    batched=True,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_prepend_static_feature_token(batch, \"ethnicity\"),\n",
    "    batched=True,\n",
    ")\n",
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e496585",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bff19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(SAVE_RAW_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4d62e",
   "metadata": {},
   "source": [
    "# Make tokenizer\n",
    "\n",
    "Adapted from https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt#building-a-wordpiece-tokenizer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_tokens_count = {\n",
    "    token for tokens in dataset[\"train\"][\"tokens\"] for token in tokens\n",
    "}\n",
    "temporal_tokens = {f\"time_diff_{i}\" for i in range(11)} | {\n",
    "    f\"year_of_birth_{i}\" for i in range(2000, 2030)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordLevel(unk_token=UNKNOWN_TOKEN))\n",
    "# Separator and end of sequence tokens are already in the dataset\n",
    "trainer = trainers.WordLevelTrainer(special_tokens=[UNKNOWN_TOKEN, PADDING_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48dd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(inferred_tokens_count | temporal_tokens, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(dataset[\"train\"][0][\"tokens\"], is_pretokenized=True)\n",
    "print(list(zip(encoding.tokens, encoding.ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe15b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_fast_tokenizer = PreTrainedTokenizerFastWithPositionIDPadding(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=UNKNOWN_TOKEN,\n",
    "    pad_token=PADDING_TOKEN,\n",
    "    sep_token=SEPARATOR_TOKEN,\n",
    "    eos_token=EOS_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sample = pretrained_fast_tokenizer(\n",
    "    dataset[\"train\"][0][\"tokens\"], is_split_into_words=True\n",
    ")\n",
    "encoded_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_fast_tokenizer.save_pretrained(SAVE_TOKENIZER_PATH)\n",
    "reloaded_tokenizer = PreTrainedTokenizerFastWithPositionIDPadding.from_pretrained(\n",
    "    SAVE_TOKENIZER_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6cf3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sample_reloaded = reloaded_tokenizer(\n",
    "    dataset[\"train\"][0][\"tokens\"], is_split_into_words=True\n",
    ")\n",
    "assert encoded_sample == encoded_sample_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48659ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "    lambda batch: pretrained_fast_tokenizer(\n",
    "        batch[\"tokens\"], is_split_into_words=True, return_token_type_ids=False\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "        \"timeline\",\n",
    "        \"timestamps\",\n",
    "        \"year_of_birth\",\n",
    "        \"sex\",\n",
    "        \"ethnicity\",\n",
    "        \"tokens\",\n",
    "    ],\n",
    ")\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ed6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in encoded_dataset[\"train\"][0].items():\n",
    "    print(key, value[:10] if type(value) == list else value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb1c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.save_to_disk(SAVE_ENCODED_DATASET_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

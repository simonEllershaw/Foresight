{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ae6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import datasets\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "# from foresight.datasets import patient_concept_stream\n",
    "# from foresight.datasets.filters import filter_by_count, filter_by_type\n",
    "# from foresight.datasets.utils import get_embeddings_for_tokens, stream_to_separate_examples, add_to_stream, \\\n",
    "#                                   remove_parents_from_stream, bucket_concepts, cleanup_stream, \\\n",
    "#                                   split_stream, add_age, get_all_splits, add_ttd, add_position_ids\n",
    "# from foresight.utils import pickle\n",
    "# from foresight.utils.cdb_utils import get_parents_map \n",
    "# from foresight.utils.stream_utils import docs2stream, calculate_counts\n",
    "# from foresight.tokenizers.simple_map_tokenizer import SimpleMapTokenizer\n",
    "# from foresight.metrics.next_concept_prediction import precision, metrics_data2df, ComputePrecisionHF\n",
    "# from foresight.utils import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c38222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import Random\n",
    "from datasets import Dataset\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8208b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foresight.tokenizers.simple_map_tokenizer_v2 import SimpleMapTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c930ab",
   "metadata": {},
   "source": [
    "## Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b27dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TIMELINES = 1000\n",
    "LETTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "NUM_TIMESTEPS = 10\n",
    "RANDOM_SEED = Random(23)\n",
    "MAX_SKIP = 10\n",
    "MAX_NUM_SAMPLES = 3\n",
    "\n",
    "SEQUENCE_LENGTH = 12\n",
    "SEPARATOR_TOKEN = \"<SEP>\"\n",
    "PADDING_TOKEN = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0faa734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    for _ in range(1000):\n",
    "        start_idx = RANDOM_SEED.randint(0, (len(LETTERS)/2))\n",
    "        skip_odd = RANDOM_SEED.choice([True, False])\n",
    "        num_samples = RANDOM_SEED.randint(1, MAX_NUM_SAMPLES)\n",
    "\n",
    "        timeline = []\n",
    "        for seq_idx in range(SEQUENCE_LENGTH):\n",
    "            char_idx = start_idx + num_samples*seq_idx\n",
    "            if skip_odd and char_idx % 2 == 1:\n",
    "                timeline.append([])\n",
    "            elif char_idx+num_samples >= len(LETTERS):\n",
    "                timeline.append([])\n",
    "            else:\n",
    "                timeline.append(list(LETTERS[char_idx:char_idx+num_samples]))\n",
    "            \n",
    "        yield(\n",
    "            {\n",
    "                \"timeline\": timeline,\n",
    "                \"start_idx\": start_idx,\n",
    "                \"skip_odd\": skip_odd,\n",
    "                \"num_samples\": num_samples,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60d346b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_generator(get_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a77b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timeline': [['M'],\n",
       "  ['N'],\n",
       "  ['O'],\n",
       "  ['P'],\n",
       "  ['Q'],\n",
       "  ['R'],\n",
       "  ['S'],\n",
       "  ['T'],\n",
       "  ['U'],\n",
       "  ['V'],\n",
       "  ['W'],\n",
       "  ['X']],\n",
       " 'start_idx': 12,\n",
       " 'skip_odd': False,\n",
       " 'num_samples': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428fcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_timeline_to_tokens(batched_samples: dict[str, list], separator: str)->dict[str, list]:\n",
    "    batched_samples[\"tokens\"] = [\n",
    "        [\n",
    "            timestep_value\n",
    "            for timestep in timeline\n",
    "            for timestep_value in [separator] + timestep\n",
    "        ]\n",
    "        for timeline in batched_samples[\"timeline\"]\n",
    "    ]\n",
    "    return batched_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c34301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['M'], ['N'], ['O'], ['P'], ['Q'], ['R'], ['S'], ['T'], ['U'], ['V']],\n",
       " ['<SEP>', 'M', '<SEP>', 'N', '<SEP>', 'O', '<SEP>', 'P', '<SEP>', 'Q'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda batch: batched_timeline_to_tokens(batch, SEPARATOR_TOKEN), batched=True)\n",
    "dataset[0][\"timeline\"][:10], dataset[0][\"tokens\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd9dec95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start_idx_12',\n",
       " 'skip_odd_False',\n",
       " 'num_samples_1',\n",
       " '<SEP>',\n",
       " 'M',\n",
       " '<SEP>',\n",
       " 'N',\n",
       " '<SEP>',\n",
       " 'O',\n",
       " '<SEP>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_insert_static_feature_token(batched_samples: dict[str, list], key:str, insert_idx:int)->dict[str, list]:\n",
    "    for idx, _ in enumerate(batched_samples[\"tokens\"]):\n",
    "        batched_samples[\"tokens\"][idx].insert(insert_idx, f\"{key}_{batched_samples[key][idx]}\")\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "dataset = dataset.map(lambda batch: batched_insert_static_feature_token(batch, \"start_idx\", insert_idx=0), batched=True)\n",
    "dataset = dataset.map(lambda batch: batched_insert_static_feature_token(batch, \"skip_odd\", insert_idx=1), batched=True)\n",
    "dataset = dataset.map(lambda batch: batched_insert_static_feature_token(batch, \"num_samples\", insert_idx=2), batched=True)\n",
    "dataset[0][\"tokens\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3aa069",
   "metadata": {},
   "source": [
    "## Add position IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b07180d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('start_idx_12', 0),\n",
       " ('skip_odd_False', 0),\n",
       " ('num_samples_1', 0),\n",
       " ('<SEP>', 1),\n",
       " ('M', 1),\n",
       " ('<SEP>', 2),\n",
       " ('N', 2),\n",
       " ('<SEP>', 3),\n",
       " ('O', 3),\n",
       " ('<SEP>', 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_add_position_ids(batched_samples: dict[str, list], separators:set[str])->dict[str, list]:\n",
    "    batched_samples[\"position_ids\"] = []\n",
    "    for tokens in batched_samples['tokens']:\n",
    "        position_ids = []\n",
    "        cnt = 0\n",
    "        for token in tokens:\n",
    "            if token in separators:\n",
    "                cnt += 1\n",
    "            position_ids.append(cnt)\n",
    "        batched_samples[\"position_ids\"].append(position_ids)\n",
    "    return batched_samples\n",
    "\n",
    "dataset = dataset.map(lambda batch: batched_add_position_ids(batch, {SEPARATOR_TOKEN}), batched=True)\n",
    "[(dataset[0][\"tokens\"][idx], dataset[0][\"position_ids\"][idx]) for idx in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7792d191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "len(dataset[\"train\"]), len(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4d62e",
   "metadata": {},
   "source": [
    "# Make tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371f82cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'<SEP>': 9600,\n",
       "         'O': 638,\n",
       "         'M': 633,\n",
       "         'Q': 577,\n",
       "         'S': 567,\n",
       "         'K': 536,\n",
       "         'U': 536,\n",
       "         'N': 495,\n",
       "         'W': 468,\n",
       "         'P': 454,\n",
       "         'I': 449,\n",
       "         'T': 443,\n",
       "         'R': 434,\n",
       "         'L': 416,\n",
       "         'V': 409,\n",
       "         'skip_odd_True': 401,\n",
       "         'skip_odd_False': 399,\n",
       "         'J': 352,\n",
       "         'X': 350,\n",
       "         'G': 343,\n",
       "         'H': 292,\n",
       "         'num_samples_1': 275,\n",
       "         'num_samples_3': 269,\n",
       "         'num_samples_2': 256,\n",
       "         'E': 250,\n",
       "         'F': 206,\n",
       "         'Y': 175,\n",
       "         'C': 166,\n",
       "         'D': 144,\n",
       "         'B': 82,\n",
       "         'start_idx_0': 65,\n",
       "         'A': 65,\n",
       "         'start_idx_12': 65,\n",
       "         'start_idx_2': 65,\n",
       "         'start_idx_10': 64,\n",
       "         'start_idx_9': 60,\n",
       "         'start_idx_3': 59,\n",
       "         'start_idx_7': 57,\n",
       "         'start_idx_8': 54,\n",
       "         'start_idx_13': 53,\n",
       "         'start_idx_5': 53,\n",
       "         'start_idx_4': 53,\n",
       "         'start_idx_11': 51,\n",
       "         'start_idx_6': 51,\n",
       "         'start_idx_1': 50})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count = Counter(token for tokens in dataset[\"train\"][\"tokens\"] for token in tokens)\n",
    "token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78cb8ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [46,\n",
       "  31,\n",
       "  28,\n",
       "  1,\n",
       "  12,\n",
       "  1,\n",
       "  13,\n",
       "  1,\n",
       "  14,\n",
       "  1,\n",
       "  15,\n",
       "  1,\n",
       "  16,\n",
       "  1,\n",
       "  17,\n",
       "  1,\n",
       "  18,\n",
       "  1,\n",
       "  19,\n",
       "  1,\n",
       "  20,\n",
       "  1,\n",
       "  21,\n",
       "  1,\n",
       "  22,\n",
       "  1,\n",
       "  23],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SimpleMapTokenizer.from_vocab(token_count.keys())\n",
    "tokenizer(dataset[\"train\"][\"tokens\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54691592",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(Path.cwd() / \"outputs\" / \"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "548e8e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f682d227954b549b3617a5bca13f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b240b2c4434544bab6102a7b4ffd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "        lambda batch: tokenizer.batch_encode(batch),\n",
    "        batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "154ed6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timeline': [['J'],\n",
       "  ['K'],\n",
       "  ['L'],\n",
       "  ['M'],\n",
       "  ['N'],\n",
       "  ['O'],\n",
       "  ['P'],\n",
       "  ['Q'],\n",
       "  ['R'],\n",
       "  ['S'],\n",
       "  ['T'],\n",
       "  ['U']],\n",
       " 'start_idx': 9,\n",
       " 'skip_odd': False,\n",
       " 'num_samples': 1,\n",
       " 'tokens': ['start_idx_9',\n",
       "  'skip_odd_False',\n",
       "  'num_samples_1',\n",
       "  '<SEP>',\n",
       "  'J',\n",
       "  '<SEP>',\n",
       "  'K',\n",
       "  '<SEP>',\n",
       "  'L',\n",
       "  '<SEP>',\n",
       "  'M',\n",
       "  '<SEP>',\n",
       "  'N',\n",
       "  '<SEP>',\n",
       "  'O',\n",
       "  '<SEP>',\n",
       "  'P',\n",
       "  '<SEP>',\n",
       "  'Q',\n",
       "  '<SEP>',\n",
       "  'R',\n",
       "  '<SEP>',\n",
       "  'S',\n",
       "  '<SEP>',\n",
       "  'T',\n",
       "  '<SEP>',\n",
       "  'U'],\n",
       " 'position_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  12],\n",
       " 'input_ids': [46,\n",
       "  31,\n",
       "  28,\n",
       "  1,\n",
       "  12,\n",
       "  1,\n",
       "  13,\n",
       "  1,\n",
       "  14,\n",
       "  1,\n",
       "  15,\n",
       "  1,\n",
       "  16,\n",
       "  1,\n",
       "  17,\n",
       "  1,\n",
       "  18,\n",
       "  1,\n",
       "  19,\n",
       "  1,\n",
       "  20,\n",
       "  1,\n",
       "  21,\n",
       "  1,\n",
       "  22,\n",
       "  1,\n",
       "  23],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffb1c303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33843c60b26f4953bf1d442b72cc78d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ebc4ee421446a782e62411f029b8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset.save_to_disk(Path.cwd() / \"outputs\" / \"encoded_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from random import Random\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foresight.tokenizers.simple_map_tokenizer_v2 import SimpleMapTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c930ab",
   "metadata": {},
   "source": [
    "## Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TIMELINES = 1000\n",
    "LETTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "NUM_TIMESTEPS = 10\n",
    "RANDOM_SEED = Random(23)\n",
    "MAX_SKIP = 10\n",
    "MAX_NUM_SAMPLES = 3\n",
    "\n",
    "SEQUENCE_LENGTH = 12\n",
    "SEPARATOR_TOKEN = \"<SEP>\"\n",
    "PADDING_TOKEN = \"<PAD>\"\n",
    "\n",
    "OUTPUT_DIR = Path.cwd() / \"outputs\"\n",
    "SAVE_TOKENIZER_PATH = OUTPUT_DIR / \"tokenizer\"\n",
    "SAVE_ENCODED_DATASET_PATH = OUTPUT_DIR / \"encoded_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    for _ in range(1000):\n",
    "        start_idx = RANDOM_SEED.randint(0, (len(LETTERS) / 2))\n",
    "        skip_odd = RANDOM_SEED.choice([True, False])\n",
    "        num_samples = RANDOM_SEED.randint(1, MAX_NUM_SAMPLES)\n",
    "\n",
    "        timeline = []\n",
    "        for seq_idx in range(SEQUENCE_LENGTH):\n",
    "            char_idx = start_idx + num_samples * seq_idx\n",
    "            if skip_odd and char_idx % 2 == 1:\n",
    "                timeline.append([])\n",
    "            elif char_idx + num_samples >= len(LETTERS):\n",
    "                timeline.append([])\n",
    "            else:\n",
    "                timeline.append(list(LETTERS[char_idx : char_idx + num_samples]))\n",
    "\n",
    "        yield (\n",
    "            {\n",
    "                \"timeline\": timeline,\n",
    "                \"start_idx\": start_idx,\n",
    "                \"skip_odd\": skip_odd,\n",
    "                \"num_samples\": num_samples,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d346b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_generator(get_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a77b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_timeline_to_tokens(\n",
    "    batched_samples: dict[str, list], separator: str\n",
    ") -> dict[str, list]:\n",
    "    batched_samples[\"tokens\"] = [\n",
    "        [\n",
    "            timestep_value\n",
    "            for timestep in timeline\n",
    "            for timestep_value in [separator] + timestep\n",
    "        ]\n",
    "        for timeline in batched_samples[\"timeline\"]\n",
    "    ]\n",
    "    return batched_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c34301",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda batch: batched_timeline_to_tokens(batch, SEPARATOR_TOKEN), batched=True\n",
    ")\n",
    "dataset[0][\"timeline\"][:10], dataset[0][\"tokens\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9dec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_insert_static_feature_token(\n",
    "    batched_samples: dict[str, list], key: str, insert_idx: int\n",
    ") -> dict[str, list]:\n",
    "    for idx, _ in enumerate(batched_samples[\"tokens\"]):\n",
    "        batched_samples[\"tokens\"][idx].insert(\n",
    "            insert_idx, f\"{key}_{batched_samples[key][idx]}\"\n",
    "        )\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_insert_static_feature_token(batch, \"start_idx\", insert_idx=0),\n",
    "    batched=True,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_insert_static_feature_token(batch, \"skip_odd\", insert_idx=1),\n",
    "    batched=True,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_insert_static_feature_token(\n",
    "        batch, \"num_samples\", insert_idx=2\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "dataset[0][\"tokens\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3aa069",
   "metadata": {},
   "source": [
    "## Add position IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b07180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_add_position_ids(\n",
    "    batched_samples: dict[str, list], separators: set[str]\n",
    ") -> dict[str, list]:\n",
    "    batched_samples[\"position_ids\"] = []\n",
    "    for tokens in batched_samples[\"tokens\"]:\n",
    "        position_ids = []\n",
    "        cnt = 0\n",
    "        for token in tokens:\n",
    "            if token in separators:\n",
    "                cnt += 1\n",
    "            position_ids.append(cnt)\n",
    "        batched_samples[\"position_ids\"].append(position_ids)\n",
    "    return batched_samples\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda batch: batched_add_position_ids(batch, {SEPARATOR_TOKEN}), batched=True\n",
    ")\n",
    "[(dataset[0][\"tokens\"][idx], dataset[0][\"position_ids\"][idx]) for idx in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "len(dataset[\"train\"]), len(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4d62e",
   "metadata": {},
   "source": [
    "# Make tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = Counter(\n",
    "    token for tokens in dataset[\"train\"][\"tokens\"] for token in tokens\n",
    ")\n",
    "token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb8ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleMapTokenizer(token_count.keys())\n",
    "encoded_sample = tokenizer.encode(dataset[\"train\"][0][\"tokens\"])\n",
    "for key, value in encoded_sample.items():\n",
    "    print(key, value[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54691592",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(SAVE_TOKENIZER_PATH)\n",
    "reloaded_tokenizer = SimpleMapTokenizer.load(SAVE_TOKENIZER_PATH)\n",
    "encoded_sample_reloaded = reloaded_tokenizer.encode(dataset[\"train\"][\"tokens\"][0])\n",
    "assert encoded_sample == encoded_sample_reloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(lambda batch: tokenizer.batch_encode(batch), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ed6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in encoded_dataset[\"train\"][0].items():\n",
    "    print(key, value[:10] if type(value) == list else value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb1c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.save_to_disk(SAVE_ENCODED_DATASET_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

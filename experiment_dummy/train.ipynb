{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from foresight.datasets.data_collator_v2 import (\n",
    "    DataCollatorForLanguageModelingMaskStaticVariables,\n",
    ")\n",
    "from foresight.models.foresight_llama import (\n",
    "    ForesightLlamaConfig,\n",
    "    ForesightLlamaForCausalLM,\n",
    ")\n",
    "from foresight.tokenizers import PreTrainedTokenizerFastWithPositionIDPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0304477",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger()\n",
    "log.handlers.clear()\n",
    "log.addHandler(logging.StreamHandler())\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path.cwd() / \"outputs\"\n",
    "SAVE_TOKENIZER_PATH = OUTPUT_DIR / \"tokenizer\"\n",
    "SAVE_ENCODED_DATASET_PATH = OUTPUT_DIR / \"encoded_dataset\"\n",
    "MODEL_LOGS_DIR = OUTPUT_DIR / \"model_logs\" / time.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "FINAL_MODEL_DIR = MODEL_LOGS_DIR / \"final_model\"\n",
    "MODEL_LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NUM_STATIC_VARIABLES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = datasets.load_from_disk(SAVE_ENCODED_DATASET_PATH)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8fb7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFastWithPositionIDPadding.from_pretrained(\n",
    "    SAVE_TOKENIZER_PATH\n",
    ")\n",
    "training_data_collator = DataCollatorForLanguageModelingMaskStaticVariables(\n",
    "    tokenizer=tokenizer, mlm=False, num_static_variables=NUM_STATIC_VARIABLES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a22d32c",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    params: dict[str, Any],\n",
    "    tokenizer: PreTrainedTokenizerFastWithPositionIDPadding,\n",
    "    max_sequence_length: int,\n",
    "):\n",
    "    print(\"get_model\", params)\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    hidden_size = params.get(\"hidden_size\", 512)\n",
    "    # From OLMo paper\n",
    "    intermediate_size = hidden_size / (8 / 3)\n",
    "    intermediate_size = round(intermediate_size / 100) * 100\n",
    "\n",
    "    config = ForesightLlamaConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        hidden_size=hidden_size,\n",
    "        intermediate_size=intermediate_size,\n",
    "        num_hidden_layers=params.get(\"num_attention_heads\", 4),\n",
    "        num_attention_heads=params.get(\n",
    "            \"num_attention_heads\", 4\n",
    "        ),  # TODO: Check if to tie these\n",
    "        num_key_value_heads=params.get(\n",
    "            \"num_attention_heads\", 4\n",
    "        ),  # TODO: Use multi-head attention\n",
    "        max_position_embeddings=max_sequence_length,\n",
    "        use_cache=False,  # TODO: Figure out how to use cache\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=-100,  # We don't use BOS token\n",
    "        sep_token_id=tokenizer.sep_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        tie_word_embeddings=False,\n",
    "        rope_theta=10000.0,  # TODO: Read up on ROPE\n",
    "        rope_scaling=None,\n",
    "        attention_bias=False,\n",
    "        attention_dropout=params.get(\"attention_dropout\", 0.0),  # Config\n",
    "    )\n",
    "\n",
    "    return ForesightLlamaForCausalLM(config)\n",
    "\n",
    "\n",
    "max_sequence_length = math.ceil(\n",
    "    max(len(sample[\"input_ids\"]) for sample in encoded_dataset[\"train\"]) * 1.2\n",
    ")\n",
    "\n",
    "get_model_lambda = lambda params: get_model(  # noqa : E731\n",
    "    params, tokenizer, max_sequence_length\n",
    ")\n",
    "trial_model = get_model_lambda(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 / 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e62c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "10000 ** (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee32ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in trial_model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113550ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dataset = DataLoader(\n",
    "    encoded_dataset[\"train\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=training_data_collator,\n",
    ")\n",
    "batch = next(iter(trial_dataset))\n",
    "trial_model(**{k: v for k, v in batch.items()}).logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ee1ea",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus_per_trial = 1\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_LOGS_DIR,  # output directory\n",
    "    no_cuda=gpus_per_trial <= 0,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=32,  # config\n",
    "    warmup_ratio=0.1,  # config\n",
    "    weight_decay=0.1,  # config\n",
    "    logging_dir=\"./logs\",\n",
    "    skip_memory_metrics=True,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_objective(metrics):\n",
    "    metrics = copy.deepcopy(metrics)\n",
    "    return metrics.pop(\"eval_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=get_model_lambda,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    data_collator=training_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92114c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bbeaebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5ba9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# log = logging.getLogger()\n",
    "# log.handlers.clear()\n",
    "# log.addHandler(logging.StreamHandler())\n",
    "# log.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78bc409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# from medcat.cat import CAT\n",
    "# from foresight.models.lucid_transformers import LucidLM2HF\n",
    "from transformers import SchedulerType, Trainer, TrainingArguments\n",
    "\n",
    "# from medcat.cdb import CDB\n",
    "from foresight.datasets.data_collator import CollataAndPad\n",
    "from foresight.datasets.data_collator_v2 import (\n",
    "    DataCollatorForLanguageModelingMaskStaticVariables,\n",
    ")\n",
    "from foresight.metrics.next_concept_prediction import (\n",
    "    ComputePrecisionHF,\n",
    "    metrics_data2df,\n",
    "    precision,\n",
    ")\n",
    "from foresight.models.custom_GPT2 import CustomGPT2Config, CustomGPT2LMHeadModel\n",
    "from foresight.tokenizers import PreTrainedTokenizerFastWithPositionIDPadding\n",
    "from foresight.tokenizers.simple_map_tokenizer import SimpleMapTokenizer\n",
    "from foresight.utils import pickle\n",
    "from foresight.metrics.timeline import TimelineMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394581c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8580feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path.cwd() / \"outputs\"\n",
    "SAVE_TOKENIZER_PATH = OUTPUT_DIR / \"tokenizer\"\n",
    "SAVE_ENCODED_DATASET_PATH = OUTPUT_DIR / \"encoded_dataset\"\n",
    "MODEL_LOGS_DIR = OUTPUT_DIR / \"model_logs\" / time.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "FINAL_MODEL_DIR = MODEL_LOGS_DIR / \"final_model\"\n",
    "\n",
    "\n",
    "NUM_STATIC_VARIABLES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bdb26f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset = datasets.load_from_disk(SAVE_ENCODED_DATASET_PATH)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb8fb7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFastWithPositionIDPadding.from_pretrained(\n",
    "    SAVE_TOKENIZER_PATH\n",
    ")\n",
    "training_data_collator = DataCollatorForLanguageModelingMaskStaticVariables(\n",
    "    tokenizer=tokenizer, mlm=False, num_static_variables=NUM_STATIC_VARIABLES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d30ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a22d32c",
   "metadata": {},
   "source": [
    "# Create GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58cc7f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"max_length\": 100,\n",
       "  \"pad_token_id\": 1\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = math.ceil(max(len(sample[\"input_ids\"]) for sample in encoded_dataset[\"train\"]) * 1.2)\n",
    "\n",
    "# Make a new model\n",
    "config = CustomGPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=max_sequence_length,\n",
    "    n_embd=16,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    bos_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.pad_token_id,\n",
    "    sep_token_id=tokenizer.sep_token_id,\n",
    ")\n",
    "model = CustomGPT2LMHeadModel(config)\n",
    "model.generation_config.max_length = max_sequence_length\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "113550ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dataset = DataLoader(\n",
    "    encoded_dataset[\"train\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=training_data_collator,\n",
    ")\n",
    "batch = next(iter(trial_dataset))\n",
    "# model(**{k:v for k, v in batch.items()}).logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ee1ea",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3ed16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimon_ellershaw\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a025533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/simon/Foresight/experiment_dummy/wandb/run-20240207_194728-4g7h85nz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/simon_ellershaw/nhs_foresight_dummy_experiments/runs/4g7h85nz' target=\"_blank\">2024_02_07_19_47_16</a></strong> to <a href='https://wandb.ai/simon_ellershaw/nhs_foresight_dummy_experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/simon_ellershaw/nhs_foresight_dummy_experiments' target=\"_blank\">https://wandb.ai/simon_ellershaw/nhs_foresight_dummy_experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/simon_ellershaw/nhs_foresight_dummy_experiments/runs/4g7h85nz' target=\"_blank\">https://wandb.ai/simon_ellershaw/nhs_foresight_dummy_experiments/runs/4g7h85nz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/simon_ellershaw/nhs_foresight_dummy_experiments/runs/4g7h85nz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f90e46a6b90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"end\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"nhs_foresight_dummy_experiments\",\n",
    "    config = config.to_dict(),\n",
    "    name = MODEL_LOGS_DIR.stem,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "974a735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_LOGS_DIR,  # output directory\n",
    "    num_train_epochs=5,  # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "    # weight_decay=1e-2,               # strength of weight decay\n",
    "    # logging_dir='./logs',            # directory for storing logs\n",
    "    # warmup_ratio=0.01,\n",
    "    learning_rate=2e-03,\n",
    "    # eval_accumulation_steps=1,\n",
    "    # gradient_accumulation_steps=16,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    # lr_scheduler_type=SchedulerType.LINEAR,\n",
    "    # use_cpu=True\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7f0c996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "timeline_metrics = TimelineMetrics(tokenizer)\n",
    "compute_metrics = lambda eval_preds: timeline_metrics.batch_compute_precision_recall_f1(\n",
    "    eval_preds, batch_size = 100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=encoded_dataset[\"train\"],  # training dataset\n",
    "    eval_dataset=encoded_dataset[\"test\"],  # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=training_data_collator,\n",
    "    # prediction_loss_only=True\n",
    "    # tokenizer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3c7ed86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='2815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 198/2815 00:04 < 00:54, 47.78 it/s, Epoch 0.35/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c3e8de6bf24c09af2c23a22d273cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.071 MB of 0.071 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▇███</td></tr><tr><td>eval/loss</td><td>█▃▂▁▁</td></tr><tr><td>eval/num_samples</td><td>▁▁▁▁▁</td></tr><tr><td>eval/precision</td><td>▁▇███</td></tr><tr><td>eval/recall</td><td>▁▆███</td></tr><tr><td>eval/runtime</td><td>▁█▁█▁</td></tr><tr><td>eval/samples_per_second</td><td>█▁▇▁█</td></tr><tr><td>eval/steps_per_second</td><td>█▁▇▁█</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▄▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▄▅▆▆▇██</td></tr><tr><td>train/learning_rate</td><td>█▆▄▃▁</td></tr><tr><td>train/loss</td><td>█▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.9787</td></tr><tr><td>eval/loss</td><td>0.05213</td></tr><tr><td>eval/num_samples</td><td>8631.0</td></tr><tr><td>eval/precision</td><td>0.95829</td></tr><tr><td>eval/recall</td><td>1.0</td></tr><tr><td>eval/runtime</td><td>0.6378</td></tr><tr><td>eval/samples_per_second</td><td>1567.873</td></tr><tr><td>eval/steps_per_second</td><td>98.776</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>2815</td></tr><tr><td>train/learning_rate</td><td>0.00025</td></tr><tr><td>train/loss</td><td>0.203</td></tr><tr><td>train/total_flos</td><td>238454387712.0</td></tr><tr><td>train/train_loss</td><td>0.60058</td></tr><tr><td>train/train_runtime</td><td>68.0561</td></tr><tr><td>train/train_samples_per_second</td><td>661.219</td></tr><tr><td>train/train_steps_per_second</td><td>41.363</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024_02_07_19_47_16</strong> at: <a href='https://wandb.ai/simon_ellershaw/nhs_foresight_dummy_experiments/runs/4g7h85nz' target=\"_blank\">https://wandb.ai/simon_ellershaw/nhs_foresight_dummy_experiments/runs/4g7h85nz</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240207_194728-4g7h85nz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()\n",
    "trainer.save_model(FINAL_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f59062d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomGPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(53, 16)\n",
       "    (wpe): Embedding(100, 16)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x GPT2Block(\n",
       "        (ln_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=16, out_features=53, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomGPT2LMHeadModel.from_pretrained(FINAL_MODEL_DIR)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dded235",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "inference_data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "batch = inference_data_collator(\n",
    "    encoded_dataset[\"test\"][:2],\n",
    ")\n",
    "batch = {k: v.to(\"cuda\") for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de7313b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27, 28, 47,  2,  3, 25,  2,  3, 24,  2,  3, 23,  2,  3, 22,  2,  3, 21,\n",
       "          2,  3, 18,  2,  3,  7,  2,  3, 15,  2,  3, 19,  2,  3, 13,  2,  3, 12,\n",
       "          2,  3,  9,  2,  3, 17,  2,  3, 16,  2,  3, 10,  2,  3,  8,  2,  3, 20,\n",
       "          2,  3, 14,  2,  3, 11,  6,  2,  3, 11,  6,  2,  3, 11,  6,  2,  3, 11,\n",
       "          6,  2,  3, 11,  6,  2,  3, 11,  6,  2,  3, 11,  6,  2,  3, 11,  6,  2,\n",
       "          3, 11,  6,  2,  3, 11,  6,  2,  3, 11],\n",
       "        [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1, 32, 28, 51,  2,  3, 21, 18,  2,  3,  7,\n",
       "         15,  2,  3, 19, 13,  2,  3, 12,  9,  2,  3, 17, 16,  2,  3, 10,  8,  2,\n",
       "          3, 20, 14,  2,  3, 11,  6,  2,  3, 11,  6,  2,  3, 11,  6,  2,  3, 11,\n",
       "          6,  2,  3, 11,  6,  2,  3, 11,  6,  2,  3, 11,  6,  2,  3, 11,  6,  2,\n",
       "          3, 11,  6,  2,  3, 11,  6,  2,  3, 11]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = model.generate(**batch).cpu()\n",
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f001e151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['num_samples_1',\n",
       "  'num_blanks_0',\n",
       "  'start_idx_7',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'H',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'I',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'J',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'K',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'L',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'M',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'N',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'O',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'P',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Q',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'R',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'S',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'T',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'U',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'V',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'W',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'X',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Y',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z'],\n",
       " ['num_samples_2',\n",
       "  'num_blanks_0',\n",
       "  'start_idx_11',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'L',\n",
       "  'M',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'N',\n",
       "  'O',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'P',\n",
       "  'Q',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'R',\n",
       "  'S',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'T',\n",
       "  'U',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'V',\n",
       "  'W',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'X',\n",
       "  'Y',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z',\n",
       "  '<EOS>',\n",
       "  '<SEP>',\n",
       "  'char_diff_0',\n",
       "  'Z']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens = [\n",
    "    [\n",
    "        token\n",
    "        for token in tokenizer.convert_ids_to_tokens(ids)\n",
    "        if token != tokenizer.pad_token\n",
    "    ]\n",
    "    for ids in output_ids\n",
    "]\n",
    "output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f6a43",
   "metadata": {},
   "source": [
    "# Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe6a39e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tune\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtune\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CLIReporter\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray'"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import PopulationBasedTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics = ComputePrecisionHF(\n",
    "    id2tkn, id2type, prediction_scope=\"age\", topk=1, start=0, batch_size=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 20\n",
    "N_GPU_PER_TRIAL = 1\n",
    "METRIC_TO_OPTIMIZE = \"eval_precision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27783bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(params):\n",
    "    torch.cuda.empty_cache()\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    config = GPT2Config(\n",
    "        vocab_size=len(embeddings),\n",
    "        n_positions=MAX_SEQ_LEN + 1,\n",
    "        n_ctx=MAX_SEQ_LEN + 1,\n",
    "        n_embd=params.get(\"n_embd\", 300),\n",
    "        n_layer=params.get(\"n_layer\", 1),\n",
    "        n_head=params.get(\"n_head\", 1),\n",
    "        bos_token_id=tkn2id[\"<PAD>\"],\n",
    "        eos_token_id=tkn2id[\"<PAD>\"],\n",
    "    )\n",
    "    model = GPT2LMHeadModel(config)\n",
    "\n",
    "    if params.get(\"load_weights\", 0):\n",
    "        model.transformer.wte.load_state_dict(\n",
    "            {\"weight\": torch.tensor(embeddings, dtype=torch.float32)}\n",
    "        )\n",
    "        model.transformer.wte.weight.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e4a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # output directory\n",
    "    num_train_epochs=5,  # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=128,  # batch size for evaluation\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir=\"./logs\",  # directory for storing logs\n",
    "    logging_steps=200,\n",
    "    eval_steps=200,\n",
    "    learning_rate=5e-5,\n",
    "    eval_accumulation_steps=1,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    skip_memory_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc2eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.n_head = 1\n",
    "training_args.n_layer = 1\n",
    "training_args.n_embd = 300\n",
    "training_args.load_weights = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_dataset = encoded_dataset[\"train\"].train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f111f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_train_dataset = tune_dataset[\"train\"]\n",
    "tune_test_dataset = tune_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    #    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=tune_train_dataset,  # training dataset\n",
    "    eval_dataset=tune_test_dataset,  # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=None,\n",
    "    model_init=get_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0356cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_config = {\n",
    "    \"num_train_epochs\": tune.choice([5]),\n",
    "    \"n_head\": tune.choice([2, 4, 6]),\n",
    "}\n",
    "scheduler = PopulationBasedTraining(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=METRIC_TO_OPTIMIZE,\n",
    "    mode=\"max\",\n",
    "    perturbation_interval=1,\n",
    "    hyperparam_mutations={\n",
    "        \"weight_decay\": tune.uniform(0.0, 0.3),\n",
    "        \"learning_rate\": tune.uniform(1e-5, 5e-5),\n",
    "        \"per_device_train_batch_size\": [16, 32, 64, 128],\n",
    "        \"n_layer\": tune.choice([2, 4, 6, 8]),\n",
    "        #       \"n_embd\": tune.choice([256, 512]),\n",
    "        \"load_weights\": tune.choice([0, 1]),\n",
    "        \"warmup_steps\": tune.choice([20, 40, 60, 100]),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    metrics = copy.deepcopy(metrics)\n",
    "    eval_precision = metrics.pop(\"eval_precision\")\n",
    "\n",
    "    return eval_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trainer.hyperparameter_search(\n",
    "    hp_space=lambda _: tune_config,\n",
    "    backend=\"ray\",\n",
    "    n_trials=NUM_TRIALS,\n",
    "    direction=\"maximize\",\n",
    "    compute_objective=compute_objective,\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": N_GPU_PER_TRIAL},\n",
    "    scheduler=scheduler,\n",
    "    keep_checkpoints_num=1,\n",
    "    checkpoint_score_attr=METRIC_TO_OPTIMIZE,\n",
    "    stop=None,\n",
    "    local_dir=RESULTS_HYPERPARAM,\n",
    "    name=\"21_May_2021\",\n",
    "    log_to_file=False,\n",
    "    loggers=None,  # (WandbLogger, ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db3fae5",
   "metadata": {},
   "source": [
    "# Saliency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ecco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db3fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ecco.LM(trainer.model, tokenizer, model_name=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 49\n",
    "print(\n",
    "    \"~~\".join(\n",
    "        [tokenizer.id2tkn[id] for id in encoded_dataset[\"test\"][ind][\"input_ids\"]]\n",
    "    )\n",
    ")\n",
    "text = \"~~\".join(\n",
    "    [tokenizer.id2tkn[id] for id in encoded_dataset[\"test\"][ind][\"input_ids\"][1:-1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28108d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = lm.generate(text, generate=10, do_sample=True, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b3898",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.saliency(style=\"detailed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb6cf5",
   "metadata": {},
   "source": [
    "# Probability prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foresight.sight import Sight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb6cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d741d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sight = Sight(tokenizer=tokenizer, device=\"cuda\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdb.name2cuis[\"muscle~pain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdb.get_name(\"pain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<ETHNICITY>~~White~~<SEX>~~Male~~<AGE>~~23~~49727002~~386661006\".split(\"~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7fc62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small with WD\n",
    "r = sight.next_concepts(\n",
    "    text, type_ids=[\"T-11\"], n=40, p_new=True, create_position_ids=False\n",
    ")\n",
    "print([cdb.get_name(x) for x in text])\n",
    "for x in r:\n",
    "    print(x[0], x[1], cdb.get_name(x[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

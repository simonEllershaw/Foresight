{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "from foresight.datasets.data_collator_v2 import (\n",
    "    DataCollatorForLanguageModelingMaskStaticVariables,\n",
    ")\n",
    "from foresight.models.custom_GPT2 import CustomGPT2Config, CustomGPT2LMHeadModel\n",
    "from foresight.tokenizers import PreTrainedTokenizerFastWithPositionIDPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0304477",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger()\n",
    "log.handlers.clear()\n",
    "log.addHandler(logging.StreamHandler())\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path.cwd() / \"outputs\"\n",
    "SAVE_TOKENIZER_PATH = OUTPUT_DIR / \"tokenizer\"\n",
    "SAVE_ENCODED_DATASET_PATH = OUTPUT_DIR / \"encoded_dataset\"\n",
    "MODEL_LOGS_DIR = OUTPUT_DIR / \"model_logs\" / time.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "FINAL_MODEL_DIR = MODEL_LOGS_DIR / \"final_model\"\n",
    "MODEL_LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NUM_STATIC_VARIABLES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = datasets.load_from_disk(SAVE_ENCODED_DATASET_PATH)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8fb7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFastWithPositionIDPadding.from_pretrained(\n",
    "    SAVE_TOKENIZER_PATH\n",
    ")\n",
    "training_data_collator = DataCollatorForLanguageModelingMaskStaticVariables(\n",
    "    tokenizer=tokenizer, mlm=False, num_static_variables=NUM_STATIC_VARIABLES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a22d32c",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    params: dict[str, Any],\n",
    "    tokenizer: PreTrainedTokenizerFastWithPositionIDPadding,\n",
    "    max_sequence_length: int,\n",
    "):\n",
    "    print(\"get_model\", params)\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    hidden_size = params.get(\"hidden_size\", 512)\n",
    "    # From OLMo paper\n",
    "    intermediate_size = hidden_size / (8 / 3)\n",
    "    intermediate_size = round(intermediate_size / 100) * 100\n",
    "\n",
    "    config = CustomGPT2Config(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        hidden_size=hidden_size,\n",
    "        intermediate_size=intermediate_size,\n",
    "        num_hidden_layers=params.get(\"num_attention_heads\", 4),\n",
    "        num_attention_heads=params.get(\n",
    "            \"num_attention_heads\", 4\n",
    "        ),  # TODO: Check if to tie these\n",
    "        num_key_value_heads=params.get(\n",
    "            \"num_attention_heads\", 4\n",
    "        ),  # TODO: Use multi-head attention\n",
    "        max_position_embeddings=max_sequence_length,\n",
    "        use_cache=False,  # TODO: Figure out how to use cache\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=-100,  # We don't use BOS token\n",
    "        sep_token_id=tokenizer.sep_token_id,\n",
    "        eos_token_id=2,  # TODO: Add DEATH token\n",
    "        tie_word_embeddings=False,\n",
    "        rope_theta=10000.0,  # TODO: Read up on ROPE\n",
    "        rope_scaling=None,\n",
    "        attention_bias=False,\n",
    "        attention_dropout=params.get(\"attention_dropout\", 0.0),  # Config\n",
    "    )\n",
    "\n",
    "    return CustomGPT2LMHeadModel(config)\n",
    "\n",
    "\n",
    "max_sequence_length = math.ceil(\n",
    "    max(len(sample[\"input_ids\"]) for sample in encoded_dataset[\"train\"]) * 1.2\n",
    ")\n",
    "\n",
    "get_model_lambda = lambda params: get_model(  # noqa : E731\n",
    "    params, tokenizer, max_sequence_length\n",
    ")\n",
    "trial_model = get_model_lambda(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee32ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in trial_model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113550ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dataset = DataLoader(\n",
    "    encoded_dataset[\"train\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=training_data_collator,\n",
    ")\n",
    "batch = next(iter(trial_dataset))\n",
    "trial_model(**{k: v for k, v in batch.items()}).logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ee1ea",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus_per_trial = 1\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_LOGS_DIR,  # output directory\n",
    "    no_cuda=gpus_per_trial <= 0,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=32,  # config\n",
    "    warmup_ratio=0.1,  # config\n",
    "    weight_decay=0.1,  # config\n",
    "    logging_dir=\"./logs\",\n",
    "    skip_memory_metrics=True,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_objective(metrics):\n",
    "    metrics = copy.deepcopy(metrics)\n",
    "    return metrics.pop(\"eval_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeline_metrics = TimelineMetrics(tokenizer)\n",
    "# compute_metrics = lambda eval_preds: timeline_metrics.batch_compute_precision_recall_f1(\n",
    "#     eval_preds, batch_size = 100\n",
    "# )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=get_model_lambda,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    data_collator=training_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_config = {\n",
    "    \"per_device_train_batch_size\": tune.choice([16, 32, 64, 128]),\n",
    "}\n",
    "\n",
    "pbt_scheduler = PopulationBasedTraining(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=\"eval_loss\",\n",
    "    mode=\"min\",\n",
    "    perturbation_interval=1,\n",
    "    hyperparam_mutations={\n",
    "        \"weight_decay\": tune.uniform(0.0, 0.3),\n",
    "        \"learning_rate\": tune.loguniform(1e-5, 1e-1),\n",
    "        \"warmup_ratio\": tune.loguniform(1e-2, 1e-1),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5231a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporter = JupyterNotebookReporter(\n",
    "    parameter_columns=[\n",
    "        \"weight_decay\",\n",
    "        \"learning_rate\",\n",
    "        \"warmup_ratio\",\n",
    "        \"per_device_train_batch_size\",\n",
    "        \"n_layer_and_heads\",\n",
    "        \"embed_dim\",\n",
    "    ],\n",
    "    metric_columns=[\"eval_loss\", \"epoch\", \"training_iteration\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d9f8d",
   "metadata": {},
   "source": [
    "Missing LM head weights is fine as they are tied together [see](https://discuss.huggingface.co/t/why-is-the-lm-head-layer-in-gpt2lmheadmodel-not-a-parameter/639)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d44f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(_temp_dir=\"/data2/tmp\")\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    hp_space=lambda _: tune_config,\n",
    "    backend=\"ray\",\n",
    "    n_trials=4,\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": gpus_per_trial},\n",
    "    scheduler=pbt_scheduler,\n",
    "    # checkpoint_config=CheckpointConfig(\n",
    "    #     num_to_keep=1,\n",
    "    #     checkpoint_score_attribute=\"training_iteration\",\n",
    "    # ),\n",
    "    # progress_reporter=reporter,\n",
    "    local_dir=str(MODEL_LOGS_DIR),\n",
    "    log_to_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59062d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomGPT2LMHeadModel.from_pretrained(FINAL_MODEL_DIR)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dded235",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "inference_data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "batch = inference_data_collator(\n",
    "    encoded_dataset[\"test\"][:2],\n",
    ")\n",
    "batch = {k: v.to(\"cuda\") for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7313b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(**batch).cpu()\n",
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f001e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens = [\n",
    "    [\n",
    "        token\n",
    "        for token in tokenizer.convert_ids_to_tokens(ids)\n",
    "        if token != tokenizer.pad_token\n",
    "    ]\n",
    "    for ids in output_ids\n",
    "]\n",
    "output_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
